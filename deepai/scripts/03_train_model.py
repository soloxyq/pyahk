#!/usr/bin/env python
# -*- coding: utf-8 -*-
"""æ­¥éª¤3: æ¨¡å‹è®­ç»ƒ - ä½¿ç”¨TensorFlow CPUè®­ç»ƒè½»é‡çº§CNN"""

import os
os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'  # å‡å°‘TensorFlowæ—¥å¿—è¾“å‡º

import sys
from pathlib import Path
import json
import numpy as np
import cv2
import tensorflow as tf
from tensorflow import keras
from tensorflow.keras import layers, models
from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau
from sklearn.model_selection import train_test_split
from collections import Counter
import matplotlib.pyplot as plt

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°è·¯å¾„
sys.path.insert(0, str(Path(__file__).parent.parent))

from deepai.config import *


def configure_device():
    """é…ç½®è®¡ç®—è®¾å¤‡"""
    print(f"\n{'='*80}")
    print(f"âš™ï¸ é…ç½®è®¡ç®—è®¾å¤‡")
    print(f"{'='*80}")
    
    if USE_GPU:
        gpus = tf.config.list_physical_devices('GPU')
        if gpus:
            try:
                # è®¾ç½®æ˜¾å­˜é™åˆ¶
                if GPU_MEMORY_LIMIT:
                    tf.config.set_logical_device_configuration(
                        gpus[0],
                        [tf.config.LogicalDeviceConfiguration(memory_limit=GPU_MEMORY_LIMIT)]
                    )
                print(f"âœ… ä½¿ç”¨GPUè®­ç»ƒ: {gpus[0].name}")
                if GPU_MEMORY_LIMIT:
                    print(f"   æ˜¾å­˜é™åˆ¶: {GPU_MEMORY_LIMIT} MB")
            except RuntimeError as e:
                print(f"âš ï¸ GPUé…ç½®å¤±è´¥: {e}")
                print(f"   å°†ä½¿ç”¨CPUè®­ç»ƒ")
        else:
            print(f"âš ï¸ æœªæ£€æµ‹åˆ°GPUè®¾å¤‡")
            print(f"   å°†ä½¿ç”¨CPUè®­ç»ƒ")
    else:
        # ç¦ç”¨GPU
        tf.config.set_visible_devices([], 'GPU')
        print(f"âœ… ä½¿ç”¨CPUè®­ç»ƒ")
        print(f"   æç¤º: å¯¹äº11ä¸ªå­—ç¬¦çš„è½»é‡çº§æ¨¡å‹,CPUè®­ç»ƒå®Œå…¨è¶³å¤Ÿ")
        print(f"   é¢„è®¡è®­ç»ƒæ—¶é—´: 1-5åˆ†é’Ÿ(å–å†³äºæ•°æ®é‡)")
    
    print(f"TensorFlowç‰ˆæœ¬: {tf.__version__}")
    print(f"Kerasç‰ˆæœ¬: {keras.__version__}")
    print(f"âœ… é…ç½®å®Œæˆ!\n")


def load_dataset(labels_path):
    """åŠ è½½æ•°æ®é›†"""
    print(f"\n{'='*80}")
    print(f"ğŸ“Š åŠ è½½æ•°æ®é›†")
    print(f"{'='*80}")
    
    with open(labels_path, 'r', encoding='utf-8') as f:
        data = json.load(f)
    
    # è¿‡æ»¤å·²æ ‡æ³¨çš„æ•°æ®
    labeled_data = [d for d in data if d.get('label') and len(d['label']) == 1]
    
    print(f"æ€»æ•°æ®: {len(data)}")
    print(f"å·²æ ‡æ³¨: {len(labeled_data)}")
    
    if len(labeled_data) < 50:
        print(f"âŒ é”™è¯¯: æ ‡æ³¨æ•°æ®ä¸è¶³(è‡³å°‘éœ€è¦50ä¸ª)")
        return None, None, None
    
    # ç»Ÿè®¡æ ‡ç­¾åˆ†å¸ƒ
    label_counts = Counter([d['label'] for d in labeled_data])
    print(f"\næ ‡ç­¾åˆ†å¸ƒ:")
    for label, count in sorted(label_counts.items()):
        print(f"  '{label}': {count} ä¸ª")
    
    # åˆ›å»ºæ ‡ç­¾æ˜ å°„
    unique_labels = sorted(label_counts.keys())
    label_to_idx = {label: idx for idx, label in enumerate(unique_labels)}
    idx_to_label = {idx: label for label, idx in label_to_idx.items()}
    
    print(f"\nç±»åˆ«æ•°: {len(unique_labels)}")
    print(f"æ ‡ç­¾æ˜ å°„: {label_to_idx}")
    
    # åŠ è½½å›¾åƒå’Œæ ‡ç­¾
    images = []
    labels = []
    
    for d in labeled_data:
        img_path = d['image_path']
        label = d['label']
        
        img = cv2.imread(img_path, cv2.IMREAD_GRAYSCALE)
        if img is None:
            continue
        
        # è°ƒæ•´åˆ°å›ºå®šå¤§å°
        img_resized = cv2.resize(img, (IMG_WIDTH, IMG_HEIGHT), interpolation=cv2.INTER_AREA)
        
        # å½’ä¸€åŒ–
        img_normalized = img_resized.astype(np.float32) / 255.0
        
        images.append(img_normalized)
        labels.append(label_to_idx[label])
    
    images = np.array(images)
    labels = np.array(labels)
    
    # æ·»åŠ é€šé“ç»´åº¦
    images = np.expand_dims(images, axis=-1)
    
    print(f"\næ•°æ®å½¢çŠ¶:")
    print(f"  å›¾åƒ: {images.shape}")
    print(f"  æ ‡ç­¾: {labels.shape}")
    print(f"âœ… åŠ è½½å®Œæˆ!\n")
    
    return images, labels, idx_to_label


def create_model(num_classes):
    """åˆ›å»ºè½»é‡çº§CNNæ¨¡å‹"""
    print(f"\n{'='*80}")
    print(f"ğŸ§  åˆ›å»ºæ¨¡å‹")
    print(f"{'='*80}")
    
    # æ•°æ®å¢å¼ºå±‚ï¼ˆæˆä¸ºæ¨¡å‹çš„ä¸€éƒ¨åˆ†ï¼ŒGPUåŠ é€Ÿï¼Œç¡®ä¿è®­ç»ƒæ¨ç†ä¸€è‡´æ€§ï¼‰
    data_augmentation = keras.Sequential([
        layers.RandomRotation(factor=AUGMENTATION['rotation_range'] / 360),
        layers.RandomZoom(height_factor=AUGMENTATION['zoom_range'], width_factor=AUGMENTATION['zoom_range']),
        layers.RandomTranslation(
            height_factor=AUGMENTATION['height_shift_range'], 
            width_factor=AUGMENTATION['width_shift_range']
        ),
    ], name='data_augmentation')
    
    model = models.Sequential([
        # æ•°æ®å¢å¼ºå±‚ï¼ˆä»…åœ¨è®­ç»ƒæ—¶æ¿€æ´»ï¼‰
        data_augmentation,
        
        # ç¬¬ä¸€å±‚å·ç§¯ - ç‰¹å¾æå–
        layers.Conv2D(32, (3, 3), activation='relu', input_shape=(IMG_HEIGHT, IMG_WIDTH, 1)),
        layers.MaxPooling2D((2, 2)),
        layers.Dropout(0.25),
        
        # ç¬¬äºŒå±‚å·ç§¯ - æ›´å¤æ‚çš„ç‰¹å¾
        layers.Conv2D(64, (3, 3), activation='relu'),
        layers.MaxPooling2D((2, 2)),
        layers.Dropout(0.25),
        
        # å…¨è¿æ¥å±‚
        layers.Flatten(),
        layers.Dense(128, activation='relu'),
        layers.Dropout(0.5),
        layers.Dense(num_classes, activation='softmax')
    ])
    
    # ç¼–è¯‘æ¨¡å‹
    model.compile(
        optimizer=keras.optimizers.Adam(learning_rate=LEARNING_RATE),
        loss='sparse_categorical_crossentropy',
        metrics=['accuracy']
    )
    
    model.summary()
    
    # è®¡ç®—å‚æ•°é‡
    total_params = model.count_params()
    print(f"\næ¨¡å‹å‚æ•°é‡: {total_params:,}")
    print(f"æ¨¡å‹å¤§å°(ä¼°ç®—): {total_params * 4 / 1024 / 1024:.2f} MB")
    print(f"âœ… æ¨¡å‹åˆ›å»ºå®Œæˆ!\n")
    
    return model


def train_model(model, X_train, y_train, X_val, y_val):
    """è®­ç»ƒæ¨¡å‹"""
    print(f"\n{'='*80}")
    print(f"ğŸš€ å¼€å§‹è®­ç»ƒ")
    print(f"{'='*80}")
    
    # æ•°æ®å¢å¼ºå·²é€šè¿‡Kerasé¢„å¤„ç†å±‚é›†æˆåˆ°æ¨¡å‹ä¸­ï¼Œæ— éœ€ImageDataGenerator
    print(f"âœ… ä½¿ç”¨Kerasé¢„å¤„ç†å±‚è¿›è¡Œæ•°æ®å¢å¼ºï¼ˆGPUåŠ é€Ÿï¼‰")
    
    # å›è°ƒå‡½æ•°
    callbacks = [
        # æ—©åœ
        EarlyStopping(
            monitor='val_loss',
            patience=EARLY_STOPPING_PATIENCE,
            restore_best_weights=True,
            verbose=1
        ),
        # ä¿å­˜æœ€ä½³æ¨¡å‹
        ModelCheckpoint(
            MODEL_SAVE_PATH,
            monitor='val_accuracy',
            save_best_only=True,
            verbose=1
        ),
        # å­¦ä¹ ç‡è¡°å‡
        ReduceLROnPlateau(
            monitor='val_loss',
            factor=0.5,
            patience=3,
            min_lr=1e-6,
            verbose=1
        )
    ]
    
    # è®­ç»ƒï¼ˆæ•°æ®å¢å¼ºå±‚å·²åœ¨æ¨¡å‹ä¸­ï¼Œç›´æ¥ä½¿ç”¨åŸå§‹æ•°æ®ï¼‰
    history = model.fit(
        X_train, y_train,
        batch_size=BATCH_SIZE,
        epochs=EPOCHS,
        validation_data=(X_val, y_val),
        callbacks=callbacks,
        verbose=1
    )
    
    print(f"\n{'='*80}")
    print(f"âœ… è®­ç»ƒå®Œæˆ!")
    print(f"{'='*80}\n")
    
    return history


def plot_training_history(history, save_path):
    """ç»˜åˆ¶è®­ç»ƒæ›²çº¿"""
    print(f"ğŸ“ˆ ç»˜åˆ¶è®­ç»ƒæ›²çº¿...")
    
    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 4))
    
    # å‡†ç¡®ç‡æ›²çº¿
    ax1.plot(history.history['accuracy'], label='è®­ç»ƒé›†')
    ax1.plot(history.history['val_accuracy'], label='éªŒè¯é›†')
    ax1.set_title('æ¨¡å‹å‡†ç¡®ç‡')
    ax1.set_xlabel('Epoch')
    ax1.set_ylabel('å‡†ç¡®ç‡')
    ax1.legend()
    ax1.grid(True)
    
    # æŸå¤±æ›²çº¿
    ax2.plot(history.history['loss'], label='è®­ç»ƒé›†')
    ax2.plot(history.history['val_loss'], label='éªŒè¯é›†')
    ax2.set_title('æ¨¡å‹æŸå¤±')
    ax2.set_xlabel('Epoch')
    ax2.set_ylabel('æŸå¤±')
    ax2.legend()
    ax2.grid(True)
    
    plt.tight_layout()
    plt.savefig(save_path)
    print(f"âœ… è®­ç»ƒæ›²çº¿å·²ä¿å­˜: {save_path}\n")


def evaluate_model(model, X_test, y_test, idx_to_label):
    """è¯„ä¼°æ¨¡å‹"""
    print(f"\n{'='*80}")
    print(f"ğŸ“Š æ¨¡å‹è¯„ä¼°")
    print(f"{'='*80}")
    
    # é¢„æµ‹
    y_pred = model.predict(X_test, verbose=0)
    y_pred_classes = np.argmax(y_pred, axis=1)
    
    # è®¡ç®—å‡†ç¡®ç‡
    accuracy = np.mean(y_pred_classes == y_test)
    print(f"æµ‹è¯•é›†å‡†ç¡®ç‡: {accuracy:.2%}")
    
    # è®¡ç®—æ¯ä¸ªç±»åˆ«çš„å‡†ç¡®ç‡
    print(f"\nå„ç±»åˆ«å‡†ç¡®ç‡:")
    for idx, label in sorted(idx_to_label.items()):
        mask = y_test == idx
        if mask.sum() > 0:
            class_acc = np.mean(y_pred_classes[mask] == y_test[mask])
            print(f"  '{label}': {class_acc:.2%} ({mask.sum()} æ ·æœ¬)")
    
    # è®¡ç®—ç½®ä¿¡åº¦ç»Ÿè®¡
    max_probs = np.max(y_pred, axis=1)
    print(f"\nç½®ä¿¡åº¦ç»Ÿè®¡:")
    print(f"  å¹³å‡: {max_probs.mean():.2%}")
    print(f"  ä¸­ä½æ•°: {np.median(max_probs):.2%}")
    print(f"  æœ€å°: {max_probs.min():.2%}")
    
    print(f"âœ… è¯„ä¼°å®Œæˆ!\n")
    
    return accuracy


def main():
    """ä¸»å‡½æ•°"""
    print(f"\n{'='*80}")
    print(f"ğŸš€ DeepAI æ¨¡å‹è®­ç»ƒæµç¨‹")
    print(f"{'='*80}\n")
    
    # é…ç½®è®¾å¤‡
    configure_device()
    
    # åŠ è½½æ•°æ®
    labels_path = os.path.join(DATA_DIR, 'labels.json')
    images, labels, idx_to_label = load_dataset(labels_path)
    
    if images is None:
        return
    
    # åˆ’åˆ†æ•°æ®é›†
    print(f"{'='*80}")
    print(f"ğŸ“Š åˆ’åˆ†æ•°æ®é›†")
    print(f"{'='*80}")
    
    X_train, X_test, y_train, y_test = train_test_split(
        images, labels, test_size=0.2, random_state=42, stratify=labels
    )
    
    X_train, X_val, y_train, y_val = train_test_split(
        X_train, y_train, test_size=0.2, random_state=42, stratify=y_train
    )
    
    print(f"è®­ç»ƒé›†: {X_train.shape[0]} æ ·æœ¬")
    print(f"éªŒè¯é›†: {X_val.shape[0]} æ ·æœ¬")
    print(f"æµ‹è¯•é›†: {X_test.shape[0]} æ ·æœ¬")
    print(f"âœ… åˆ’åˆ†å®Œæˆ!\n")
    
    # åˆ›å»ºæ¨¡å‹
    num_classes = len(idx_to_label)
    model = create_model(num_classes)
    
    # è®­ç»ƒæ¨¡å‹
    history = train_model(model, X_train, y_train, X_val, y_val)
    
    # ç»˜åˆ¶è®­ç»ƒæ›²çº¿
    history_plot_path = os.path.join(MODELS_DIR, 'training_history.png')
    plot_training_history(history, history_plot_path)
    
    # è¯„ä¼°æ¨¡å‹
    evaluate_model(model, X_test, y_test, idx_to_label)
    
    # ä¿å­˜æ ‡ç­¾æ˜ å°„
    label_map_path = os.path.join(MODELS_DIR, 'label_map.json')
    with open(label_map_path, 'w', encoding='utf-8') as f:
        json.dump(idx_to_label, f, indent=2, ensure_ascii=False)
    print(f"æ ‡ç­¾æ˜ å°„å·²ä¿å­˜: {label_map_path}")
    
    print(f"\n{'='*80}")
    print(f"âœ… è®­ç»ƒæµç¨‹å®Œæˆ!")
    print(f"{'='*80}")
    print(f"\nç”Ÿæˆçš„æ–‡ä»¶:")
    print(f"  æ¨¡å‹æ–‡ä»¶: {MODEL_SAVE_PATH}")
    print(f"  æ ‡ç­¾æ˜ å°„: {label_map_path}")
    print(f"  è®­ç»ƒæ›²çº¿: {history_plot_path}")
    print(f"\nä¸‹ä¸€æ­¥: è¿è¡Œè¯„ä¼°è„šæœ¬")
    print(f"  python deepai/scripts/04_evaluate_model.py")
    print(f"{'='*80}\n")


if __name__ == "__main__":
    main()
